
/* 
 * Clinton Cario 10/7/2015
 */
import groovy.json.*
hostname = java.net.InetAddress.getLocalHost().getHostName();

// =============================
// Model Parameters 
// ---------

// Mutation data information
//!! TODO: move params.num_mutations here and make it secific to real mutations; cleanup file splits with sensible defaults 
params.filename                = 'feature_test.vcf'    // The name of the mutation file. This file must be found in the DATA_DIR/mutations folder and be a valid vcf file or a tab seperated file in ICGC format (ssm_open.tsv)
params.datatag                 = 'feature_test'        // The name of the current dataset (will name the database this and create corresponding folder in the final destination)
//params.copy_number_file        = 'copy_number.bed'   // For the copy number feature if desired (database specific)

// Database parameters
params.database_ip             = 'localhost'           // The URL of the database which to write data
params.database_port           = 3306                  // The database port (3306 for MySQL/MemSQL, 5432 for PostgreSQL)
params.database_username       = 'orchid'              // The username to access the created database
params.database_password       = 'orchid_flower'       // The password to access the created database

// Simulated mutation information
//!! TODO: add params.num_simulated functionality as below, seperating real and simulated import numbers 
params.add_simulated           = false                   // 'true' or 'false'; Whether to add simulated variants to the database (a number equal to the number of real, by default)
params.use_simulated_cache     = true                    // 'true' or 'false'; If add_simulated is true, whether to regenerate simulated mutations, or use the last set
//params.num_simulated = -1 // The number of simulated mutations to add to the database or -1 to add an equal number to real
params.use_kegg                = false                   // 'true' or 'false'; Whether to use kegg cancer pathway mutation annotation or not. Will generate fresh tables from KEGG REST API or use a cached database copy

// =============================
// Execution Environments
// ---------
// Similar to nextflow profiles, but uses the hostname to determine values
// You can use these to have nextflow change parameters depending on if this code is run
//   on different machines (a local machine vs. a cluster, for example)
switch ( hostname ) {
    case "local":
      process.executor               = 'local'
      process.errorStrategy          = 'retry'
      process.maxRetries             = 3
      executor.queueSize             = 8
      env.ROOT                       = System.getProperty("user.dir")
      env.DATA_DIR                   = System.getProperty("user.dir")+'/data'  // Or an absolute path to the orchid data/ directory
      env.CODE_DIR                   = System.getProperty("user.dir")+'/code'  // Or an absolute path to the orchid code/ directory
      break

    case "cluster":

      process.executor               = 'pbs'
      process.memory                 = '4G'
      process.errorStrategy          = 'retry'
      process.maxRetries             = 3
      executor.queueSize             = 15
      env.ROOT                       = System.getProperty("user.dir")
      env.LD_LIBRARY_PATH            = '/software/lib'                         // In case you need to import custom libraries
      env.DATA_DIR                   = System.getProperty("user.dir")+'/data'  // Or an absolute path to the orchid data/ directory
      env.CODE_DIR                   = System.getProperty("user.dir")+'/code'  // Or an absolute path to the orchid code/ directory
      break

    default:
      process.executor               = 'local'
      process.errorStrategy          = 'retry'
      env.ROOT                       = System.getProperty("user.dir")
      env.DATA_DIR                   = System.getProperty("user.dir")+'/data'  // Or an absolute path to the orchid data/ directory
      env.CODE_DIR                   = System.getProperty("user.dir")+'/code'  // Or an absolute path to the orchid code/ directory
      break
}


// =============================
// Advanced Parameters
// ---------
// These don't have to be modified
params.chunk_num               = 15
params.num_mutations           = -1 // The number of mutations to add from the input file and to simulate (if requested);
params.chunk_size              = Math.ceil((params.num_mutations == -1 ? 50000 : Math.ceil(params.num_mutations/params.chunk_num))).toInteger()
params.chromosomes             = [1..22,'X','Y'].flatten()
params.mutation_table_name     = 'ssm'
params.consequence_table_name  = 'consequence'
params.acceptable_consequences = [
                                   '3_prime_UTR_variant',       '5_prime_UTR_premature_start_codon_gain_variant',   '5_prime_UTR_variant', 
                                   'downstream_gene_variant',   'exon_variant',                                     'initiator_codon_variant', 
                                   'intergenic_region',         'intragenic_variant',                               'intron_variant', 
                                   'missense_variant',          'splice_acceptor_variant',                          'splice_donor_variant', 
                                   'splice_region_variant',     'start_lost',                                       'stop_gained', 
                                   'stop_lost',                 'stop_retained_variant',                            'synonymous_variant', 
                                   'upstream_gene_variant' ,
                                 ]
env.DATABASE                   = "mysql://${params.database_username}:${params.database_password}@${params.database_ip}:${params.database_port}/${params.datatag}"
// ============================================================================



// =============================
// Database and Path Information 
// ---------
// Don't modify these unless you have custom paths for installed software or you know what you're doing

// Mysql variables for database connection on the command line
env.MYSQL_USER    = env.DATABASE.split('//')[1].split(':')[0]
env.MYSQL_PWD     = env.DATABASE.split(':')[2].split('@')[0]
env.MYSQL_IP      = env.DATABASE.split('@')[1].split(':')[0]
env.MYSQL_PORT    = env.DATABASE.split(':')[-1].split('/')[0]
env.MYSQL_DB      = env.DATABASE.split('/')[-1]  // No longer part of the connection string 


// snpEff Variables
env.SNPEFF_CODE_DIR  = env.CODE_DIR + "/external/snpEff"
env.SNPEFF_CMD       = "java -Xmx4g -jar ${env.SNPEFF_CODE_DIR}/snpEff.jar -verbose -noStats -sequenceOntology -geneId -t -noNextProt -noMotif GRCh37.75"

// Mutation Simulator
env.SIM_CODE_DIR  = env.CODE_DIR + "/external/simulator"
env.SIM_CMD       = env.SIM_CODE_DIR + "/apply_parameters.py -p ${env.SIM_CODE_DIR}/Ensembl_Compara_v66_EPO_6primate/"
env.SIM_CACHE     = env.DATA_DIR + "/simulator"

// Data variables
env.HG19_DIR      = env.DATA_DIR + "/hg19"
env.INPUT_FILE    = env.DATA_DIR + "/mutations/${params.filename}"
env.COUNT_FILE    = env.DATA_DIR + "/mutations/${params.filename}.count"
env.CCM_FILE      = env.DATA_DIR + "/mutations/${params.copy_number_file}"

// Features
env.FEATURE_DIR   = env.DATA_DIR + "/features"



// This map defines the features to use and how to process them
features = [:]


// =============================
// Feature Configuration 
// ---------
// These parameters are possible feature keys and control how features are processed using orchid_db

// At a minimum, the following should be provided:

/// Required?     Feature Type     Name                                   Default                                     Description
/// =========     ============     =======                                =========                                  ===============
//      Y            All           processor                                None                                     Can be 'numeric' (bed), 'categorical' (bed), 'tabix' (numeric), 'wig' (wig), or 'database' which will determine which annotate process will be used for this feature. Other values should have a corresponding user-defined process in the annotation nextflow file.
//      Y            All           feature_type                             None                                     Whether this feature has values that are 'floats' (decimal numbers), 'integers' (whole numbers), 'categories', or 'booleans' (T/F values). Provide ['float', 'integer', 'category', or 'boolean'].
//      Y*           All           location or file                         None                                     The location of the parent directory containing bed files when subfeatures are present, or the name of a specific file that serves as reference annotation data for a feature. If providing a custom annotate.nf processor script, this is not needed. 
//
// Required by orchid_db but have sensible defaults (meaning not required here):
//
//      N            All           source_type                              flatfile                                 Specifies how orchid_db should process an annotation file, as a 'flatfile' with mutation id and feature values as rows (delimited by [delimited]), as a 'fasta' file with sequence information (mutation ids as sequence names), or from a 'database' (use [source_connection], [source_table], and [source_value_column] to specify how).
//      N            All           connection                               defined above                            The database connection string if different from information defined above.
//      N            All           destination_table                        feature_[feature name]                   The name of the destination table to populate this feature.
//      N            All           destination_id_column                    [mutation_table]_id                      The primary key column name of [destination_table]. 
//      N            All           destination_value_column                 [feature name]                           The column name in [destination_table] for the feature values.
//      N            All           destination_value_sql                    VARCHAR(255)                             The SQL type of inserted values. Defaults to a type related to [feature_type].
//
// Specific to bed features
//
//      N            bed/wig       sub_features                             None                                     Some features have multiple sub features, like cell-line specific values. This option can be used to parse individual sub-features in one go. Simply specify the 'location' (above) of the parent directory and put sub features in a list with the file name (excluding suffixes).
//      N            bed/wig       strategy                                 max                                      For bed files, how multiple annotation values for a variant should be handled [mean, median, max, etc... (see bedops bedmap documentation)].
//
// Specific to tabix features
//
//      N            tabix         preprocessor                             None                                     A shell/awk command that will preprocess a mutation query file to be used by the tabix command. Input and output names are automatically provided as the last two parameters.
//      N            tabix         postprocessor                            None                                     A shell/awk command that will postprocess tabix lookup output into a format required by the orchid_db (rows: id [tab] value). Input and output names are automatically provided as the last two parameters.
//
// Specific to database features:
//
//      Y            database      source_table                             None                                     The source table containing data to be used to make a feature table.
//      N            database      source_value_column                      [feature name]                           When annotating the database with a flatfile or other database/table, which column orchid_db should use as the feature value. Can be numeric (for the flatfile option) or a column name (for the database option).
//
// Other orchid_db parameters:
//      N            All           source_id_column                          0                                        When annotating the database with a flatfile, which column orchid_db should use as the mutation id column.
//      N            bed/wig/tabix source_value_column                       1                                        When annotating the database with a flatfile or other database/table, which column orchid_db should use as the feature value. Can be numeric (for the flatfile option) or a column name (for the database option).
//      N            bed/wig/tabix delimiter                                '\t'                                      How flatfile columns are delimited.
//      N            All           acceptable                               None                                      If specified, orchid_db will only annotate mutations with these categories for a given feature flatfile.
//      N            All           id_processor                             None                                      If specified, orchid_db will first apply this python function to id values before querying the database.
//      N            All           sequence_processor                       None                                      If specified, orchid_db will first apply this python function to feature sequences before annotating mutations e.g. convert sequences to uppercase (fasta).
//      N            All           value_processor                          None                                      If specified, orchid_db will first apply this python function to feature values before annotating mutations (flatfile).


// ======= Trinucleotide Context ===========
// This is a special feature that is annotated with the custom 'context' processor in annotation.nf
features['context'] = 
[
  processor     : 'context',
  feature_type  : 'category',
  source_type   : 'fasta',
  seq_processor : 'lambda k: k.upper()',  // Tells orchid_db to preprocess all fasta sequence entries to uppercase
  acceptable    :  
    [
        'A[C>A]A','A[C>A]C','A[C>A]G','A[C>A]T','C[C>A]A','C[C>A]C','C[C>A]G','C[C>A]T',
        'G[C>A]A','G[C>A]C','G[C>A]G','G[C>A]T','T[C>A]A','T[C>A]C','T[C>A]G','T[C>A]T',
        'A[C>G]A','A[C>G]C','A[C>G]G','A[C>G]T','C[C>G]A','C[C>G]C','C[C>G]G','C[C>G]T',
        'G[C>G]A','G[C>G]C','G[C>G]G','G[C>G]T','T[C>G]A','T[C>G]C','T[C>G]G','T[C>G]T',
        'A[C>T]A','A[C>T]C','A[C>T]G','A[C>T]T','C[C>T]A','C[C>T]C','C[C>T]G','C[C>T]T',
        'G[C>T]A','G[C>T]C','G[C>T]G','G[C>T]T','T[C>T]A','T[C>T]C','T[C>T]G','T[C>T]T',
        'A[T>A]A','A[T>A]C','A[T>A]G','A[T>A]T','C[T>A]A','C[T>A]C','C[T>A]G','C[T>A]T',
        'G[T>A]A','G[T>A]C','G[T>A]G','G[T>A]T','T[T>A]A','T[T>A]C','T[T>A]G','T[T>A]T',
        'A[T>G]A','A[T>G]C','A[T>G]G','A[T>G]T','C[T>G]A','C[T>G]C','C[T>G]G','C[T>G]T',
        'G[T>G]A','G[T>G]C','G[T>G]G','G[T>G]T','T[T>G]A','T[T>G]C','T[T>G]G','T[T>G]T',
        'A[T>C]A','A[T>C]C','A[T>C]G','A[T>C]T','C[T>C]A','C[T>C]C','C[T>C]G','C[T>C]T',
        'G[T>C]A','G[T>C]C','G[T>C]G','G[T>C]T','T[T>C]A','T[T>C]C','T[T>C]G','T[T>C]T',
    ]  // Allowable contexts (all 96 possible trinucleotide contexts)
]





/* 
 *
 * Example feature definitions that you can use once a feature is downloaded
 *
 *

// For pantherdb
features['panther'] = 
[
  processor           : 'database',
  feature_type        : 'category',
  source_table        : 'process_gene',
  source_value_column : 'process',
  key_col             : 'mutation_id'
  //file                : 'panther.sql'
]

 processor           : 'database',
  feature_type        : 'category',
  source_table        :  params.consequence_table_name,
  source_value_column : 'consequence_type',  // Will use the feature name if not specified

// ======= Sources in Tabix format ===========
// Numeric, super yuge datafiles
// The tabix command is used for variant annotation but requires some postprocessing  

// CADD: a tool for scoring the deleteriousness of single nucleotide variants as well as insertion/deletions variants in the human genome.
// http://krishna.gs.washington.edu/download/CADD/v1.3/whole_genome_SNVs.tsv.gz
// Verified 1-based indexed tabix file ("1-based" from https://www.nature.com/ng/journal/v46/n3/extref/ng.2892-S1.pdf)
features['cadd'] = 
[
  processor     : 'tabix',
  feature_type  : 'float',
  file          :  env.FEATURE_DIR + '/CADD_whole_genome_SNVs.tsv.gz',
  postprocessor : 'awk \'BEGIN{FS="\\t"; OFS="\\t"} NR==FNR{c[$1_$2_$3_$4]=$6;next};c[$1_$2_$4_$5]{print $6,c[$1_$2_$4_$5]}\'',
]


// CADD, raw scores
// http://krishna.gs.washington.edu/download/CADD/v1.3/whole_genome_SNVs.tsv.gz
// Verified 1-based indexed tabix file ("1-based" from https://www.nature.com/ng/journal/v46/n3/extref/ng.2892-S1.pdf)
features['cadd_raw'] = 
[
  processor     : 'tabix',
  feature_type  : 'float',
  file          :  env.FEATURE_DIR + '/CADD_whole_genome_SNVs.tsv.gz',
  postprocessor : 'awk \'BEGIN{FS="\\t"; OFS="\\t"} NR==FNR{c[$1_$2_$3_$4]=$5;next};c[$1_$2_$4_$5]{print $6,c[$1_$2_$4_$5]}\'',
]


// DANN: a deep learning approach for annotating the pathogenicity of genetic variants
// https://cbcl.ics.uci.edu/public_data/DANN/data/DANN_whole_genome_SNVs.tsv.bgz
// Verified 1-based indexed tabix file ("1-based" https://cbcl.ics.uci.edu/public_data/DANN/readme)
features['dann'] = 
[
  processor     : 'tabix',
  feature_type  : 'float',
  file          :  env.FEATURE_DIR + '/DANN_whole_genome_SNVs.tsv.bgz',
  postprocessor : 'awk \'BEGIN{FS="\\t"; OFS="\\t"} NR==FNR{c[$1_$2_$3_$4]=$5;next};c[$1_$2_$4_$5]{print $6,c[$1_$2_$4_$5]}\'', 
]


// FunSeq2: Somatic cancer variants prioritization scores
// 1-based indexing (verified by ensembl lookup of feature file)
// Spot check on 5/11/2017
// http://archive.gersteinlab.org/funseq2/hg19_wg_score.tsv.gz
// Tested 1-based indexed tabix file
features['funseq2'] = 
[
  processor     : 'tabix',
  feature_type  : 'float',
  file          :  env.FEATURE_DIR + '/funseq2_whole_genome_SNVs.tsv.gz',
  preprocessor  : 'awk \'BEGIN{FS="\\t"; OFS="\\t"} {$1="chr"$1; print $1,$2,$4,$5,$6}\'',
  postprocessor : 'awk \'BEGIN{FS="\\t"; OFS="\\t"} NR==FNR{c[$1_$2_$3]=$5;next};c[$1_$2_$3]{print $5,c[$1_$2_$3],$1,$2,$3}\'',

]


// PhyloP: Nuclotide evolutionary conservation scores within the primate lineage 
// http://hgdownload.cse.ucsc.edu/goldenpath/hg19/phyloP46way/primates/
// Proper 0-based indexed bed file from UCSC, downloaded and converted to tabix with code/etc/get_phyloP.sh
// pre/post processor commands convert to/from 1-based indexing
features['phylop'] = 
[
  processor     : 'tabix',
  feature_type  : 'float',
  file          :  env.FEATURE_DIR + '/phyloP46way.bed.gz',
  preprocessor  : 'awk \'BEGIN{FS="\\t"; OFS="\\t"} {$1="chr"$1; print $1,$2-1,$4,$5,$6}\'',
  postprocessor : 'awk \'BEGIN{FS="\\t"; OFS="\\t"} NR==FNR{c[$1_$3]=$5;next};c[$1_$2]{print $5,c[$1_$2],$1,$2+1,$3}\'',
]

// ======= Sources with a Single Numeric Track ===========
// Numeric, in bed format
// Since these are numeric, there is no need to featurize these tracks

// DNase I Hypersensitivity Sites
// https://genome.ucsc.edu/cgi-bin/hgTrackUi?hgsid=591894485_zzymUZLuFbGNWWfY6HtpRSZUjy5L&c=chr18&g=wgEncodeRegDnaseClustered
// wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeRegDnaseClustered/wgEncodeRegDnaseClusteredV3.bed.gz
// Proper 0-based indexed bed file from UCSC
features['dnase'] = 
[
  processor     : 'bed',
  feature_type  : 'integer',
  file          :  env.FEATURE_DIR + '/dnase.bed',
]

// GWAS hits
// wget https://www.ebi.ac.uk/gwas/api/search/downloads/full
// awk 'BEGIN{FS="\t";OFS="\t"} ($12~/^[0-9]+$/) && ($13~/^[0-9]+$/){print "chr"$12, $13-1, $13, $21}' full > gwas_hg38.bed
// Liftover to hg19: https://genome.ucsc.edu/cgi-bin/hgLiftOver, saved as gwas.bed
// Tested 0-based indexed bed file
features['gwas'] = 
[
  processor     : 'bed',
  feature_type  : 'boolean',
  strategy      : 'none',
  file          :  env.FEATURE_DIR + '/gwas.bed',
]


// ReMap: An integrative ChIP-seq analysis of regulatory elements
// http://tagc.univ-mrs.fr/remap/download/All/nrPeaks_all.bed.gz
// Public+ENCODE Merged Peaks
// Assumed 0-based indexed bed file
features['remap'] = 
[
  processor     : 'bed',
  feature_type  : 'integer',
  file          :  env.FEATURE_DIR + '/remap.bed',
]


// wgRNA: This track displays positions of four different types of RNA in the human genome:
//    * precursor forms of microRNAs (pre-miRNAs)
//    * C/D box small nucleolar RNAs (C/D box snoRNAs)
//    * H/ACA box snoRNAs
//    * small Cajal body-specific RNAs (scaRNAs)
// https://genome.ucsc.edu/cgi-bin/hgTrackUi?db=hg19&g=wgRna
// Proper 0-based indexed bed file from UCSC
features['wgrna'] = 
[
  processor     : 'bed',
  feature_type  : 'boolean',
  file          :  env.FEATURE_DIR + '/wgrna.bed',
]


// TargetScanS: TargetScan predicts biological targets of miRNAs by searching for the presence of 8mer, 7mer, and 6mer sites that match the seed region of each miRNA (Lewis et al., 2005). 
// http://www.targetscan.org/
// Proper 0-based indexed bed file from UCSC
features['targetscans'] = 
[
  processor     : 'bed',
  feature_type  : 'integer',
  file          :  env.FEATURE_DIR + '/targetscans.bed',
]



// ======= Sources with Multiple Tracks ===========
// Numeric. The DB super enhancer cell lines columns 

// DBSuper: An integrated and interactive database of super-enhancers.
// http://bioinfo.au.tsinghua.edu.cn/dbsuper/
// http://bioinfo.au.tsinghua.edu.cn/dbsuper/data/bed/hg19/all_hg19_bed.zip
// Proper 0-based indexed bed file
features['dbsuper'] = 
[
  processor     : 'bed',
  feature_type  : 'integer',
  location      :  env.FEATURE_DIR + '/dbsuper',
  subfeatures   :  [ 'all' ], 
]


// Encode Tracks from the project page, version 2
// https://www.encodeproject.org/data/annotations/v2/
// Converted from bigBed to bed
// Proper 0-based indexed bed file
features['encode'] = 
[
  processor     : 'bed',
  feature_type  : 'integer',
  location      :  env.FEATURE_DIR + '/encode',
  subfeatures   :  [ 'distal_dnase', 'proximal_dnase', 'distal_h3k27ac', 'distal_h3k4me1', 'distal_h3k9', 'proximal_h3k4me', 'proximal_h3k9ac', 'distal_tfbs', 'proximal_tfbs' ],
]


// RFECS: human enhancer tracks (Random forest generated enhancer sites for different human cell types)
// https://www.encodeproject.org/publications/3cae12ef-5327-4222-8e46-b14898803c2f/
// http://yuelab.org/mouseENCODE/downloads/human_enhancers/
features['rfecs'] = 
[
  processor     : 'bed',
  feature_type  : 'float',
  location      :  env.FEATURE_DIR + '/rfecs',
  subfeatures   :  [ 'Gm12878','H1hesc','Helas3','Hepg2','Hmec','Hsmm','Huvec','K562','Nha','Nhdfad','Nhek','Nhlf' ], 
]


// Segmentation: Each segment belongs to one of a few specific genomic "states" which is assigned an intuitive label. 
// http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeAwgSegmentation/
features['segmentation'] = 
[
  processor     : 'categorical',
  feature_type  : 'category',
  location      :  env.FEATURE_DIR + '/segmentation',
  subfeatures   :  [ 'Gm12878','H1hesc','Helas3','Hepg2','Huvec','K562' ], 
]

// Wig: Any file in wig format can be included like this:
features['wig_example'] = 
[
  processor     : 'wig',
  feature_type  : 'float',
  file          :  env.FEATURE_DIR + '/wig_example_file.wig',
]


// ======= Database Features ===========
// Features that are already annotated in the database during population, but need to be made into feature tables
features['consequence_type'] = 
[
  processor           : 'database',
  feature_type        : 'category',
  source_table        :  params.consequence_table_name,
  source_value_column : 'consequence_type',  // Will use the feature name if not specified
  acceptable          :  params.acceptable_consequences,
]


features['impact'] = 
[
  processor           : 'database',
  feature_type        : 'category',
  source_table        :  params.consequence_table_name,
]


features['transcript_biotype'] = 
[
  processor           : 'database',
  feature_type        : 'category',
  source_table        :  params.consequence_table_name,
]


features['aa_class_change'] = 
[
  processor           : 'database',
  feature_type        : 'category',
  source_table        :  params.consequence_table_name,
]


features['kegg_pathways'] = [
  processor           : 'database',
  feature_type        : 'category',
  source_table        : 'kegg_cancer_gene_pathway',
  source_value_column : 'pathway_name',  // Will use the feature name if not specified
]



// ======= MISC Non-standard Features ===========
// These are special features that are annotated with their own custom processors (defined in annotate.nf)
// Whether a mutation falls into a cancer gene
features['kegg_cancer_gene'] = 
[
  processor                : 'kegg-cancer',
  feature_type             : 'boolean',
  destination_value_column : 'is_cancer_gene',
]


// How frequent a mutation is within the dataset
features['frequency'] = 
[
  processor             : 'frequency',
  feature_type          : 'integer',
  destination_value_sql : "INT DEFAULT 1",
]
*/


if (params.copy_number_file){
  // Mean copy number variation across donors with this mutation 
  features['copy_number'] = 
  [
    processor     : 'cnv',
    feature_type  : 'float',
    file          :  env.CCM_FILE,
  ]
}




//!!!!!!!!!! Do not modify anything below !!!!!!!!!!!!!
// =============================
// Annotation data
// ---------
// Make a data structures more accessible by the annotation script, namely by making an annotatation object with the following 
//  dictionary keys that map to a list of features
//    Processor type in annotation script corresponds to 'processor' type of features above. 
//      General Features:
//        tabix       => annotateTabixFeatures (numeric)
//        bed         => annotateBed (numeric)
//        wig         => annotateBed (numeric, after conversion during the population phase)
//        categorical => annotateCategoricalFeatures (categorical)
//        database    => annotateDatabasedFeatures (of database column type)
//      Custom Features:
//        cnv         => annotateCNV 
//        context     => annotateContext
//        kegg        => annotateKegg
//        frequency   => annotateFrequency


params.to_convert    = []
params.annotation    = [:]
params.meta_features = [:]

// For each feature, process its parameters
features.each{ feature, vals ->

  // REQUIRED PARAMETERS FOR PROCESSING
  // (reading)
  location      = vals.remove('location')                  // a directory with many feature files --or-- 
  file          = vals.remove('file')                      // the name of a feature file
  // (processing)
  processor     = vals.remove('processor')                 // tabix, numeric (bed), categorical (bed), or a custom type
  feature_type  = vals.remove('feature_type')              // the feature type, integer, float, or categorical (shortcut for sql_type)
  // (writing)
  table         = vals.remove('destination_table')         // the name of the feature table to populate
  column        = vals.remove('destination_value_column')  // the column in the feature table to populate
  sql_type      = vals.remove('destination_value_sql')     // the sql type of feature values


  // SENSIBLE DEFAULTS FOR REQUIRED PARAMETERS (if not specified above)
  // (reading)
  location = (location!=[:]) ? location : env.FEATURE_DIR
  file     = (file!=[:]) ? file : env.FEATURE_DIR+"/${feature}.${processor=='tabix' ? 'tabix' : 'bed'}" // Try to guess the feature file name from the feature name
  // (processing)
  processor = (processor!=[:]) ? processor : 'bed'  // bed is the default processor 
  // (writing)
  table = (table!=[:]) ? table : "feature_${feature}" // default feature table name
  column = (column!=[:]) ? column : feature
  if (sql_type==[:]) // If no sql_type is specified, determine it from the feature type
  {
    if (feature_type == 'integer')
      sql_type = 'INT DEFAULT NULL'
    else if (feature_type == 'float')
      sql_type = 'FLOAT DEFAULT NULL'
    else if (feature_type == 'category')
      sql_type = 'VARCHAR(255) DEFAULT NULL'
    else if (feature_type == 'boolean')
      sql_type = 'TINYINT(1) DEFAULT 0'
  }
  vals['destination_value_sql'] = sql_type   // Put it back as JSON


  // OTHER SENSIBLE DEFAULTS THAT CAN'T BE SET BY AUTOMATICALLY BY orchid_db
  // Use the mutation id as the feature table id if the feature annotation table id is unspecified 
  // (orchid_db uses [feature]_id, which will not work with this workflow)
  dsd_id = vals.remove('destination_id_column')
  dst_id = (dst_id != [:]) ? dst_id : "${params.mutation_table_name}_id"
  vals['destination_id_column'] = dst_id

  // BUILD THE ANNOTATION OBJECT
  // If a feature of this processing type hasn't been added to the annotation object yet, initialize it
  if (!params.annotation[processor])
    params.annotation[processor] = []
  // For a tabix processed feature 
  if (processor=='tabix')
  {
    preprocessor  = vals.remove('preprocessor')
    postprocessor = vals.remove('postprocessor')
    params.annotation[processor].add([ 
      file          : file, 
      table         : table,
      column        : column, 
      preprocessor  : (preprocessor == [:]) ? null : preprocessor,
      postprocessor : (postprocessor == [:]) ? null : postprocessor,
      params        : new JsonBuilder(vals) // JSONify all other passed parameters for orchid_db
    ])
    // Add this feature data to the meta data for population into the database
    params.meta_features[feature] = [
      processor     : processor,
      type          : feature_type,
      table         : table,
      column        : column, 
    ]
  }
  // For a database feature
  else if (processor=='database')
  {
    src_tbl    = vals.remove('source_table')
    src_col    = vals.remove('source_value_column')
    src_col    = src_col == [:] ? feature : src_col
    acceptable = vals.remove('acceptable')
    params.annotation[processor].add([ 
      source_table        : src_tbl,
      source_column       : src_col,
      table               : table,
      column              : column, 
      acceptable          : (acceptable == [:]) ? "IS NOT NULL" : "IN ('${acceptable.join("', '")}')",
      params              : new JsonBuilder(vals) // JSONify all other passed parameters for orchid_db
    ])
    params.meta_features[feature] = [
      processor     : processor,
      type          : feature_type,
      table         : table,
      column        : column, 
    ]
  }
  // For bed, and wig, and all other features
  else
  {
    // Get strategy and parse the argument
    def strategy = vals.remove('strategy')
    switch (strategy) 
    {
      case null:
          strategy = '--max';
          break;
      case "none":
          strategy = '';
          break;
      default:
          strategy = "--${strategy}";
          break;
    }

    // If subfeatures are defined (only for wig/bed)
    subfeatures = vals.remove('subfeatures')
    if (subfeatures!=[:] && (processor=='bed' || processor=='wig'))
    {
      subfeatures.each {
        // Handle a wig subfeature by converting to bed and changing to a bed processor
        if (processor=='wig'){
          params.to_convert.add([old_file: "${location}/${it}.wig", new_file:"${location}/${it}.bed"])
          processor = 'bed'
        }

        params.annotation[processor].add([ 
          file      : "${location}/${it}.bed", 
          table     : table,
          column    : "${column}_${it}", 
          strategy  : strategy,
          params    : new JsonBuilder(vals) // JSONify all other passed parameters for orchid_db
        ])
        // Add this sub feature data to the meta data for population into the database
        if (!params.meta_features[feature])
          params.meta_features[feature] = []
        params.meta_features[feature].add([
          processor     : processor,
          type          : feature_type,
          table         : table,
          column        : "${column}_${it}", 
        ])

      }
    }
    // No subfeatures are defined, just populate
    else
    {
      // Convert the wig file name to its bed equivalent, which should exist after the populate script finishes and before annotate
      if (processor=='wig'){
        old_file = new File(file)
        full_path = old_file.getParentFile()
        new_file = full_path.toString()+"/"+base(old_file, '.bed')
        params.to_convert.add([old_file: file, new_file: new_file])
        file = new_file
        processor='bed'
      }
      // Copy feature parameters into the annotation object to be used by the annotation scripts
      params.annotation[processor].add([ 
        file      : file,
        table     : table,
        column    : column, 
        strategy  : strategy, 
        params    : new JsonBuilder(vals) // JSONify all other passed parameters for orchid_db
      ]) 
      params.meta_features[feature] = [
        processor     : processor,
        type          : feature_type,
        table         : table,
        column        : column, 
      ]

    }
  }
}
//import static groovy.json.JsonOutput.*
//println prettyPrint(toJson(params.annotation))
//println prettyPrint(toJson(params.to_convert))

// For metadata
params.meta_params   = new JsonBuilder([
  file:               params.filename, 
  mutation_table:     params.mutation_table_name, 
  consequence_table:  params.consequence_table_name,
  consequence_types:  params.acceptable_consequences
])
params.meta_features = new JsonBuilder(params.meta_features)
params.meta_params   = params.meta_params.toString().replaceAll("\"", "\\\\\"")
params.meta_features = params.meta_features.toString().replaceAll("\"", "\\\\\"")

def base(file, ext = "") {
  name = file.name
  name = name.take(name.lastIndexOf('.'))
  //name = name.take(name.indexOf('-'))
  name = name+ext
  return name
}
